
<title>
Deep Learning
</title>
</head>

<body><h2 align="center">COS : Deep Learning (Fall 2020)</h2>
<table border="0" align="center" cellpadding="0" cellspacing="8">

<tbody><tr valign="top">
    <td><b>Logistics:</b></td>
	<td> Tue/Thu 2:00 - 3:30<br>
	     GDC 4.304<br>
       Columbia University <br>
        Unique Number: 53295<br>
	Course web page:
	<a href=".">
	https://magiclinux.github.io/deeplearning20/
	</a>
	<br>
	</td>
</tr>

<tr valign="top">
    <td><b>Professor:</b></td>  
    <td><a href="https://scholar.google.com/citations?user=yDZct7UAAAAJ&hl=en">Zhao Song</a><br>
        Email: <a href="mailto:magic.linuxkde@gmail.com"><tt>magic.linuxkde@gmail.com</tt></a><br>
        Office: GDC 4.510<br>
        Office Hours: Tuesday 3:30-5, Wednesday 3-4
</td></tr>

<tr valign="top">
    <td><b>TAs:</b></td>  
    <td><a href="http://www.cs.columbia.edu/~binghuip/">Binghui Peng</a><br>
        Email: <a href="mailto:bp2601@columbia.edu"><tt>bp2601@columbia.edu</tt></a><br>
        Office: XXX<br>
        Office Hours: XXX
</td></tr>

<tr valign="top">
    <td><b>Useful References:</b></td>
    <!--
      abcdefghijklmnopqrstuvwxyz
    -->
	<td>Courses that have reasonable overlaps
	include 
  <ol>
    <li> <a href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a> (Princeton) <a href="https://www.cs.princeton.edu/courses/archive/spring20/cos598C/"> Deep Learning for Natural Language Processing </a> </li>
    <li> <a href="http://people.csail.mit.edu/costis/">Constantinos Daskalakis</a> and <a href="https://people.csail.mit.edu/madry/">Aleksander Madry</a> (MIT) <a href="https://people.csail.mit.edu/madry/6.883/"> Science of Deep Learning </a> </li>
    <li> <a href="https://users.cs.duke.edu/~rongge/">Rong Ge</a> (Duke) <a href="https://users.cs.duke.edu/~rongge/teaching.html">Algorithmic Aspects of Machine Learning</a> </li>
    <li> <a href="https://sites.google.com/view/cjin/home">Chi Jin</a> (Princeton) <a href="https://sites.google.com/view/cjin/ele524">Foundations of Reinforcement Learning</a> </li>
    <li> <a href="http://rasmuskyng.com">Rasmus Kyng</a> (ETH Zurich) <a href="https://kyng.inf.ethz.ch/courses/AGAO20/"> Advanced Graph Algorithms and Optimization </a> </li>
    <li> <a href="http://yintat.com">Yin Tat Lee</a> (University of Washington) <a href="http://yintat.com/teaching/cse535-winter20/">Theory of Optimization and Continuous Algorithms</a></li>
    <li> <a href="https://jerryzli.github.io"> Jerry Li </a> (University of Washington) <a href="https://jerryzli.github.io/robust-ml-fall19.html"> Robustness in Machine Learning</a></li>
    <li> <a href="https://www.andrew.cmu.edu/user/yuanzhil/"> Yuanzhi Li </a> (Carnegie Mellon University) <a href="http://www.andrew.cmu.edu/user/yuanzhil/cov.html">Convex Optimization</a></li>
    <li> <a href="http://pages.cs.wisc.edu/~yliang/"> Yingyu Liang </a> (University of Wisconsin) <a href="https://www.cs.princeton.edu/courses/archive/spring16/cos495/">Introduction to Deep Learning</a> </li>
    <li> <a href="http://people.csail.mit.edu/moitra/">Ankur Moitra</a> (MIT) <a href="http://people.csail.mit.edu/moitra/408.html"> Algorithmic Aspects of Machine Learning </a> </li>
    <li> <a href="https://scholar.google.com/citations?user=mG4imMEAAAAJ&hl=en"> Andrew Ng </a> (Coursea) <a href="https://www.coursera.org/specializations/deep-learning"> Deep Learning </a>
    <li> <a href="https://www.stat.berkeley.edu/~jsteinhardt/">Jacob Steinhardt</a> (UC Berkeley) <a href="https://www.stat.berkeley.edu/~jsteinhardt/stat260/index.html"> Robust Statistics </a> </li>
    <li> <a href="http://web.stanford.edu/~sidford/">Aaron Sidford</a> (Stanford) <a href="http://web.stanford.edu/~sidford/sp17_opt_theory.html"> Introduction to Optimization Theory</a> </li>
    <li> <a href="https://www.cc.gatech.edu/~vempala/"> Santosh Vempala </a> (Georgia Tech) <a href="https://santoshv.github.io/2020CS6550/contalgos.html"> Continuous Algorithms: Optimization and Sampling </a> </li>
  </ol>

	</td>
</tr>

<tr valign="top">
   <td><b>Content:</b></td>
	<td>
          This course will take a deeper look into deep learning. In particular, we will provide some sort of understanding of deep learning from several perspectives:
          <ul>
            <li> What is deep learning ? </li>
            <li> Why is it working ? </li>
            <li> Do we trust it ? </li>
            <li> How to improve the accuracy ? </li>
            <li> How to speed up the training ? </li>
            <li> How to make the it more robust ? </li>
            <li> How to make the it more secure ? </li>
          </ul>
</td></tr>

<tr valign="top">
  <td><b>Connections to other fields</b></td>
  <td>
      <ul>
        <li> Convex optimization </li>
        <li> Dynamic data-structure </li>
        <li> Compressive sensing/Sparse recovery </li>
        <li> Cryptography </li>
        <li> Complexity </li>
      </ul>
</td></tr>

<tr valign="top">
   <td><b>Problem Sets:</b></td>
   <td>
     <ol>
       <li><a href="psets/sublinear-ps1.pdf">Problem Set 1</a>.  Due September 23.  </li>
       <li><a href="psets/sublinear-ps2.pdf">Problem Set 2</a>.  Due October 9.  </li>
     </ol>
</td></tr>
<tr valign="top">
   <td><b>Lectures:</b></td>
   <td>
     <ol>
       <li>Thursday, August 28.  Basic Introduction.  [<a href="notes/l1.pdf">Lecture notes
       (pdf)</a> <a href="notes/l2.tex">(tex)</a>]</li>



<!--
       <li>Tuesday, XXX X.  Description.  [<a href="notes/lX.pdf">Lecture notes (pdf)</a> <a href="notes/lX.tex">(tex)</a>]</li>
       <li>Thursday, XXX X.  Description.  [<a href="notes/lX.pdf">Lecture notes (pdf)</a> <a href="notes/lX.tex">(tex)</a>]</li>
-->
     </ol>

     The tentative outline for the rest of the course is as follows:
   <ul>
     <li>InstaHide, [paper], <a href="https://www.youtube.com/watch?v=eETYBl0GQsA">[video]</a>, <a href="https://github.com/Hazelsuko07/InstaHide_Challenge">[challenge]</a>, [blog] </li>
     
   </ul></td>
</tr>

<tr valign="top">
    <td><b>Prerequisites:</b></td>
    <td>
      You've heard about deep learning.
      <!--
      Mathematical maturity and comfort with undergraduate algorithms and
      basic probability.  Ideally also familiarity with linear algebra.
      -->
    </td>
</tr>

<tr valign="top">
     <td><b>Grading:</b></td>
     <td>40%: Homework<br>
         30%: Final project<br>
         20%: Scribing lectures<br>
	       10%: Participation
	 </td>
 </tr>

<tr valign="top">
    <td><b>Scribing:</b></td>
    <td>In each class, one student will be assigned to take notes.
      These notes should be written up in
      a <a href="lec-template.tex">standard LaTeX format</a> before
      the next class.
    </td></tr>

<tr valign="top">
    <td><b>Homework <br>policy:</b></td>
	<td>
          There will be a homework assignment roughly every two
          weeks.<br><br>

	<em>Collaboration policy</em>: You are encouraged to
        collaborate on homework.  However, you must write up your own
        solutions.  You should also state the names of those you
        collaborated with on the first page of your submission.
        </td></tr>

<tr valign="top">
    <td><b>Final project:</b></td>
	<td>
          In lieu of a final exam, students will perform final
          projects.  These may be done individually or in groups of
          2-3.  An ideal final project would perform a piece of
          original research in a topic related to the course.  Failing
          that, one may perform a literature survey covering several
          research papers in the field.

          <p>Students will present their results to the class during
          the last week of classes.  The final paper will be due on
          the scheduled final exam day.
        </p></td></tr>


<tr valign="top">
<td><b>References</b></td>
<td> 
  Chess
  <ol>
    <li> [W17] A Brief History of Deep Blue, IBM's Chess Computer. Mental Floss. <a href="https://www.mentalfloss.com/article/503178/brief-history-deep-blue-ibms-chess-computer">[link]</a> Kevin Warwick </li>
  </ol>
  GO
  <ol>
    <li> Mastering the game of Go with deep neural networks and tree search. Nature 2016. [arXiv] Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc </li>
    <li> Mastering the game of go without human knowledge. Nature 2017. [arXiv] Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian </li>
  </ol>
  Dota
  <ol>
    <li> [BBC+19] Dota 2 with large scale deep reinforcement learning. <a href="https://arxiv.org/pdf/1912.06680.pdf">[arXiv]</a> Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and Debiak, Przemyslaw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris </li>
  </ol>
  Texas holdem poker
  <ol>
    <li> [BSM17] Libratus: The superhuman ai for no-limit poker. IJCAI 2017. [arXiv] Noam Brown, Tuomas Sandholm, and Strategic Machine </li>
    <li> [San17] Super-human ai for strategic reasoning: Beating top pros in heads-up no-limit texas hold'em. IJCAI 2017. [arXiv] Tuomas Sandholm </li>
    <li> [BS17] Safe and nested subgame solving for imperfect-information games. NIPS 2017. [arXiv] Noam Brown and Tuomas Sandholm </li>
    <li> [BS19] Solving imperfect-information games via discounted regret minimization. AAAI 2019. [arXiv] Noam Brown and Tuomas Sandholm </li>
  </ol>

  Convergence
    <ol>
      <li> [ZSJ+17] Recovery Guarantees for One-hidden-layer Neural Networks. ICML 2017. <a href="https://arxiv.org/pdf/1706.03175.pdf">[arXiv]</a> Kai Zhong, Zhao Song, Prateek Jain, Peter L. Bartlett and Inderjit S. Dhillon </li>
      <li> [ZSD17] Learning Non-overlapping Convolutional Neural Networks with Multiple Kernels. Manuscript. <a href="https://arxiv.org/pdf/1711.03440.pdf">[arXiv]</a> Kai Zhong, Zhao Song and Inderjit Dhillon </li>
      <li> [ALS19a] A convergence theory for deep learning via over-parameterization. NeurIPS 2019. <a href="https://arxiv.org/pdf/1811.03962.pdf">[arXiv]</a> Zeyuan Allen-Zhu, Yuanzhi Li and Zhao Song </li>
      <li> [ALS19b] On the convergence rate of training recurrent neural networks. NeurIPS 2019. <a href="https://arxiv.org/pdf/1810.12065.pdf">[arXiv]</a> Zeyuan Allen-Zhu, Yuanzhi Li and Zhao Song </li>
      <li> [SY19] Quadratic Suffices for Over-parametrization via Matrix Chernoff Bound. Manuscript. <a href="https://arxiv.org/pdf/1906.03593.pdf">[arXiv]</a> Zhao Song and Xin Yang </li>
      <li> [GCL+19] Convergence of adversarial training in overparametrized neural networks. NeurIPS 2019. <a href="https://arxiv.org/pdf/1906.07916.pdf">[arXiv]</a> Ruiqi Gao, Tianle Cai, Haochuan Li, Cho-Jui Hsieh, Liwei Wang, and Jason D Lee. 
      <li> [ZPD+20] Over-parameterized Adversarial Training: An Analysis Overcoming the Curse of Dimensionality. <a href="https://arxiv.org/pdf/2002.06668.pdf">[arXiv]</a> Yi Zhang, Orestis Plevrakis, Simon S. Du, Xingguo Li, Zhao Song and Sanjeev Arora
    </ol>
    Speedup
    <ol>
      <li> [CLS19] Solving linear program in the current matrix multiplication time. STOC 2019. <a href="https://arxiv.org/pdf/1810.07896.pdf">[arXiv]</a> Michael Cohen, Yin Tat Lee and Zhao Song </li>
      <li> [LSZ19] Solving Empirical Risk Minimization in the Current Matrix Multiplication Time. COLT 2019. <a href="https://arxiv.org/pdf/1905.04447.pdf">[arXiv]</a> Yin Tat Lee, Zhao Song and Qiuyi Zhang </li>
      <li> [BPSW20] Training (Overparametrized) Neural Networks in Near-Linear Time. Manuscript. <a href="https://arxiv.org/pdf/2006.11648.pdf">[arXiv]</a> Jan van den Brand, Binghui Peng, Zhao Song and Omri Weinstein </li>
      <li> [JSWZ20] Faster Dynamic Matrix Inverse for Faster LPs. Manuscript. <a href="https://arixv.org/pdf/2004.07470.pdf">[arXiv]</a> Shunhua Jiang, Zhao Song, Omri Weinstein and Hengjie Zhang</li>
      <li> [ACSS20] Algorithms and Hardness for Linear Algebra on Geometric Graphs. FOCS 2020 [arXiv] Josh Alman, Timothy Chu, Aaron Schild and Zhao Song </li>
    </ol>
    Security
    <ol>
      <li> [HSLA20] InstaHide: Instace-hiding Schemes for Private Distributed Learning. ICML 2020. [arXiv]. Yangsibo Huang, Zhao Song, Kai Li and Sanjeev Arora
    </ol>
    Compressive sensing
    <ol>
      <li> [NS19] Stronger L2/L2 Compressed Sensing; Without Iterating. STOC 2020. <a href="https://arxiv.org/pdf/1903.02742.pdf">[arXiv]</a> Vasileios Nakos and Zhao Song </li>
      <li> [NSW19] (Nearly) Sample-Optimal Sparse Fourier Transform in Any Dimension; RIPless and Filterless. FOCS 2020. <a href="https://arxiv.org/pdf/1909.11123.pdf">[arXiv]</a> Vasileios Nakos and Zhao Song and Zhengyu Wang </li>
    </ol>
  </td>
</tr>

<tr valign="top">
    <td><b>
Students with <br>
Disabilites:
    </b></td>  
    <td>Any student with a documented disability (physical or
    cognitive) who requires academic accommodations should contact the
    Services for Students with Disabilities area of the Office of the
    Dean of Students at 471-6259 (voice) or 471-4641 (TTY for users
    who are deaf or hard of hearing) as soon as possible to request an
    official letter outlining authorized accommodations.
</td></tr>




</tbody></table>

<hr>
</body></html>

<title>
Deep Learning
</title>
</head>

<body><h2 align="center">COS : Deep Learning (Fall 2020)</h2>
<table border="0" align="center" cellpadding="0" cellspacing="8">

<tbody><tr valign="top">
    <td><b>Logistics:</b></td>
	<td> Tue/Thu 2:00 - 3:30<br>
	     GDC 4.304<br>
       Columbia University <br>
        Unique Number: 53295<br>
	Course web page:
	<a href=".">
	https://magiclinux.github.io/deeplearning20/
	</a>
	<br>
	</td>
</tr>

<tr valign="top">
    <td><b>Professor:</b></td>  
    <td><a href="https://scholar.google.com/citations?user=yDZct7UAAAAJ&hl=en">Zhao Song</a><br>
        Email: <a href="mailto:magic.linuxkde@gmail.com"><tt>magic.linuxkde@gmail.com</tt></a><br>
        Office: GDC 4.510<br>
        Office Hours: Tuesday 3:30-5, Wednesday 3-4
</td></tr>

<tr valign="top">
    <td><b>TAs:</b></td>  
    <td><a href="http://www.cs.columbia.edu/~binghuip/">Binghui Peng</a><br>
        Email: <a href="mailto:bp2601@columbia.edu"><tt>bp2601@columbia.edu</tt></a><br>
        Office: XXX<br>
        Office Hours: XXX
</td></tr>

<tr valign="top">
    <td><b>Useful References:</b></td>
    <!--
      abcdefghijklmnopqrstuvwxyz
    -->
	<td>Courses that have reasonable overlaps
	include 
  <ol>
    <li> <a href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a> (Princeton) <a href="https://www.cs.princeton.edu/courses/archive/spring20/cos598C/"> Deep Learning for Natural Language Processing </a> </li>
    <li> <a href="http://people.csail.mit.edu/costis/">Constantinos Daskalakis</a> and <a href="https://people.csail.mit.edu/madry/">Aleksander Madry</a> (MIT) <a href="https://people.csail.mit.edu/madry/6.883/"> Science of Deep Learning </a> </li>
    <li> <a href="https://users.cs.duke.edu/~rongge/">Rong Ge</a> (Duke) <a href="https://users.cs.duke.edu/~rongge/teaching.html">Algorithmic Aspects of Machine Learning</a> </li>
    <li> <a href="https://sites.google.com/view/cjin/home">Chi Jin</a> (Princeton) <a href="https://sites.google.com/view/cjin/ele524">Foundations of Reinforcement Learning</a> </li>
    <li> <a href="http://rasmuskyng.com">Rasmus Kyng</a> (ETH Zurich) <a href="https://kyng.inf.ethz.ch/courses/AGAO20/"> Advanced Graph Algorithms and Optimization </a> </li>
    <li> <a href="http://yann.lecun.com">Yann LeCun</a> (NYU) <a href="https://cs.nyu.edu/~yann/2010f-G22-2565-001/index.html">Machine Learning and Pattern Recognition</a>
    <li> <a href="http://yintat.com">Yin Tat Lee</a> (University of Washington) <a href="http://yintat.com/teaching/cse535-winter20/">Theory of Optimization and Continuous Algorithms</a></li>
    <li> <a href="https://jerryzli.github.io"> Jerry Li </a> (University of Washington) <a href="https://jerryzli.github.io/robust-ml-fall19.html"> Robustness in Machine Learning</a></li>
    <li> <a href="https://www.cs.princeton.edu/~li/"> Kai Li </a> (Princeton) <a href="https://www.cs.princeton.edu/courses/archive/spring20/cos598D/general.html"> Systems and Machine Learning </a>
    <li> <a href="https://www.andrew.cmu.edu/user/yuanzhil/"> Yuanzhi Li </a> (Carnegie Mellon University) <a href="http://www.andrew.cmu.edu/user/yuanzhil/cov.html">Convex Optimization</a></li>
    <li> <a href="http://pages.cs.wisc.edu/~yliang/"> Yingyu Liang </a> (University of Wisconsin) <a href="https://www.cs.princeton.edu/courses/archive/spring16/cos495/">Introduction to Deep Learning</a> </li>
    <li> <a href="http://people.csail.mit.edu/moitra/">Ankur Moitra</a> (MIT) <a href="http://people.csail.mit.edu/moitra/408.html"> Algorithmic Aspects of Machine Learning </a> </li>
    <li> <a href="https://scholar.google.com/citations?user=mG4imMEAAAAJ&hl=en"> Andrew Ng </a> (Coursea) <a href="https://www.coursera.org/specializations/deep-learning"> Deep Learning </a>
    <li> <a href="https://www.stat.berkeley.edu/~jsteinhardt/">Jacob Steinhardt</a> (UC Berkeley) <a href="https://www.stat.berkeley.edu/~jsteinhardt/stat260/index.html"> Robust Statistics </a> </li>
    <li> <a href="http://web.stanford.edu/~sidford/">Aaron Sidford</a> (Stanford) <a href="http://web.stanford.edu/~sidford/sp17_opt_theory.html"> Introduction to Optimization Theory</a> </li>
    <li> <a href="https://www.cc.gatech.edu/~vempala/"> Santosh Vempala </a> (Georgia Tech) <a href="https://santoshv.github.io/2020CS6550/contalgos.html"> Continuous Algorithms: Optimization and Sampling </a> </li>
  </ol>

	</td>
</tr>

<tr valign="top">
   <td><b>Content:</b></td>
	<td>
          This course will take a deeper look into deep learning. In particular, we will provide some sort of understanding of deep learning from several perspectives:
          <ul>
            <li> What is deep learning ? </li>
            <li> Why is it working ? </li>
            <li> Do we trust it ? </li>
            <li> How to improve the accuracy ? </li>
            <li> How to speed up the training ? </li>
            <li> How to make the it more robust ? </li>
            <li> How to make the it more secure ? </li>
          </ul>
</td></tr>

<tr valign="top">
  <td><b>Connections to other fields</b></td>
  <td>
      <ul>
        <li> Convex optimization </li>
        <li> Dynamic data-structure </li>
        <li> Compressive sensing/Sparse recovery </li>
        <li> Cryptography </li>
        <li> Complexity </li>
      </ul>
</td></tr>

<tr valign="top">
   <td><b>Problem Sets:</b></td>
   <td>
     <ol>
       <li><a href="psets/sublinear-ps1.pdf">Problem Set 1</a>.  Due September 23.  </li>
       <li><a href="psets/sublinear-ps2.pdf">Problem Set 2</a>.  Due October 9.  </li>
     </ol>
</td></tr>
<tr valign="top">
   <td><b>Lectures:</b></td>
   <td>
     <ol>
       <li>Thursday, August 28.  Basic Introduction.  [<a href="notes/l1.pdf">Lecture notes
       (pdf)</a> <a href="notes/l2.tex">(tex)</a>]</li>



<!--
       <li>Tuesday, XXX X.  Description.  [<a href="notes/lX.pdf">Lecture notes (pdf)</a> <a href="notes/lX.tex">(tex)</a>]</li>
       <li>Thursday, XXX X.  Description.  [<a href="notes/lX.pdf">Lecture notes (pdf)</a> <a href="notes/lX.tex">(tex)</a>]</li>
-->
     </ol>

     The tentative outline for the rest of the course is as follows:
   <ul>
     <li>InstaHide, [arXiv], <a href="https://proceedings.icml.cc/static/paper_files/icml/2020/2735-Paper.pdf">[conf]</a>, <a href="https://www.youtube.com/watch?v=eETYBl0GQsA">[video]</a>, <a href="https://github.com/Hazelsuko07/InstaHide_Challenge">[challenge]</a>, [blog] </li>
     
   </ul></td>
</tr>

<tr valign="top">
    <td><b>Course proposal:</b></td>
    <td>
      <a href="./proposal/main.pdf"> pdf </a>
    </td>
</tr>


<tr valign="top">
    <td><b>Prerequisites:</b></td>
    <td>
      You've heard about deep learning.
      <!--
      Mathematical maturity and comfort with undergraduate algorithms and
      basic probability.  Ideally also familiarity with linear algebra.
      -->
    </td>
</tr>

<tr valign="top">
     <td><b>Grading:</b></td>
     <td>45%: Homework, 3 homeworks in total, 15% each
         <br>
         40%: Project report: First proposal 10%, Second report 10%, the final one 20%
         <br>
         15%: Scribing lectures (15% per lecture; if you do 0, then add 15% to Project report, i.e., 15%+15%+25%; if you do 2, then subtract 15% from Project report, i.e., 5%+5%+15%.)
         <br>
	 </td>
 </tr>

<tr valign="top">
    <td><b>Scribing:</b></td>
    <td>In each class, one student will be assigned to take notes.
      These notes should be written up in
      a <a href="lec-template.tex">standard LaTeX format</a> before
      the next class.
    </td></tr>

<tr valign="top">
    <td><b>Homework <br>policy:</b></td>
	<td>
          There will be a homework assignment roughly every 2~3
          weeks.<br><br>

	<em>Collaboration policy</em>: You are encouraged to
        collaborate on homework.  However, you must write up your own
        solutions.  You should also state the names of those you
        collaborated with on the first page of your submission.
        </td></tr>

<tr valign="top">
    <td><b>Final project:</b></td>
	<td>
          In lieu of a final exam, students will perform final
          projects.  These may be done individually or in groups of
          2-3.  An ideal final project would perform a piece of
          original research in a topic related to the course.  Failing
          that, one may perform a literature survey covering several
          research papers in the field.

          <p>Students will present their results to the class during
          the last week of classes.  The final paper will be due on
          the scheduled final exam day.
        </p></td></tr>


<tr valign="top">
<td><b>References</b></td>
<td> 
  Chess
  <ol>
    <li> [W17] A Brief History of Deep Blue, IBM's Chess Computer. Mental Floss. <a href="https://www.mentalfloss.com/article/503178/brief-history-deep-blue-ibms-chess-computer">[link]</a> Kevin Warwick </li>
  </ol>
  GO
  <ol>
    <li> [SHM+16] Mastering the game of Go with deep neural networks and tree search. Nature 2016. <a href="https://www.nature.com/articles/nature16961">[journal]</a> David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam and Marc Lanctot </li>
    <li> [SSS+17] Mastering the game of go without human knowledge. Nature 2017. <a href="https://www.nature.com/articles/nature24270">[journal]</a> David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai and Adrian Bolton </li>
    <li> Google DeepMind Challenge Match : Lee Sedol vs AlphaGO. <a href="https://www.youtube.com/watch?v=vFr3K2DORc8">[Match 1]</a> <a href="https://www.youtube.com/watch?v=l-GsfyVCBu0">[Match 2]</a> <a href="https://www.youtube.com/watch?v=qUAmTYHEyM8">[Match 3]</a> <a href="https://www.youtube.com/watch?v=yCALyQRN3hw">[Match 4]</a> <a href="https://www.youtube.com/watch?v=mzpW10DPHeQ">[Match 5]</a> </li>
    <li> Google DeepMind Challenge Match : Ke Jie vs AlphaGO <a href="https://www.youtube.com/watch?v=Z-HL5nppBnM">[Match 1]</a> <a href="https://www.youtube.com/watch?v=1U1p4Mwis60">[Match 2]</a> <a href="https://www.youtube.com/watch?v=ru0E7N0-kFE">[Match 3]</a> </li>
    <li> Legendary players and DeepMind's AlphaGO explore the mysteries of Go together. <a href="https://www.youtube.com/watch?v=V-_Cu6Hwp5U">[video]</a> </li> 
  </ol>
  Dota
  <ol>
    <li> [BBC+19] Dota 2 with large scale deep reinforcement learning. <a href="https://arxiv.org/pdf/1912.06680.pdf">[arXiv]</a> Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme and Chris Hesse</li>
  </ol>
  Texas holdem poker
  <ol>
    <li> [BSM17] Libratus: The Superhuman AI for No-Limit poker. IJCAI 2017. <a href="https://www.ijcai.org/Proceedings/2017/0772.pdf">[conf]</a> Noam Brown, Tuomas Sandholm </li>
    <li> [San17] Super-human AI for Strategic Reasoning: Beating Top Pros in Heads-Up No-Limit Texas Hold'em. IJCAI 2017. <a href="https://www.ijcai.org/Proceedings/2017/0004.pdf">[conf]</a> Tuomas Sandholm </li>
    <li> [BS17] Safe and nested subgame solving for imperfect-information games. NIPS 2017. <a href="https://arxiv.org/pdf/1705.02955.pdf">[arXiv]</a> Noam Brown and Tuomas Sandholm </li>
    <li> [BS19] Solving imperfect-information games via discounted regret minimization. AAAI 2019. <a href="https://arxiv.org/pdf/1809.04040.pdf">[arXiv]</a> Noam Brown and Tuomas Sandholm </li>
  </ol>

  Convergence
    <ol>
      <li> [ZSJ+17] Recovery Guarantees for One-hidden-layer Neural Networks. ICML 2017. <a href="https://arxiv.org/pdf/1706.03175.pdf">[arXiv]</a> Kai Zhong, Zhao Song, Prateek Jain, Peter L. Bartlett and Inderjit S. Dhillon </li>
      <li> [ZSD17] Learning Non-overlapping Convolutional Neural Networks with Multiple Kernels. Manuscript. <a href="https://arxiv.org/pdf/1711.03440.pdf">[arXiv]</a> Kai Zhong, Zhao Song and Inderjit Dhillon </li>
      <li> [DZPS19] Gradient descent provably optimizes over-parameterized neural networks. ICLR 2019 <a href="https://arxiv.org/pdf/1810.02054.pdf">[arXiv]</a> Simon S Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh</li>
      <li> [ALS19a] A convergence theory for deep learning via over-parameterization. NeurIPS 2019. <a href="https://arxiv.org/pdf/1811.03962.pdf">[arXiv]</a> Zeyuan Allen-Zhu, Yuanzhi Li and Zhao Song </li>
      <li> [ALS19b] On the convergence rate of training recurrent neural networks. NeurIPS 2019. <a href="https://arxiv.org/pdf/1810.12065.pdf">[arXiv]</a> Zeyuan Allen-Zhu, Yuanzhi Li and Zhao Song </li>
      <li> [SY19] Quadratic Suffices for Over-parametrization via Matrix Chernoff Bound. Manuscript. <a href="https://arxiv.org/pdf/1906.03593.pdf">[arXiv]</a> Zhao Song and Xin Yang </li>
      <li> [GCL+19] Convergence of adversarial training in overparametrized neural networks. NeurIPS 2019. <a href="https://arxiv.org/pdf/1906.07916.pdf">[arXiv]</a> Ruiqi Gao, Tianle Cai, Haochuan Li, Cho-Jui Hsieh, Liwei Wang, and Jason D Lee
      <li> [ZPD+20] Over-parameterized Adversarial Training: An Analysis Overcoming the Curse of Dimensionality. <a href="https://arxiv.org/pdf/2002.06668.pdf">[arXiv]</a> Yi Zhang, Orestis Plevrakis, Simon S. Du, Xingguo Li, Zhao Song and Sanjeev Arora
    </ol>
    Speedup
    <ol>
      <li> [CLS19] Solving linear program in the current matrix multiplication time. STOC 2019. <a href="https://arxiv.org/pdf/1810.07896.pdf">[arXiv]</a> Michael Cohen, Yin Tat Lee and Zhao Song </li>
      <li> [LSZ19] Solving Empirical Risk Minimization in the Current Matrix Multiplication Time. COLT 2019. <a href="https://arxiv.org/pdf/1905.04447.pdf">[arXiv]</a> Yin Tat Lee, Zhao Song and Qiuyi Zhang </li>
      <li> [BPSW20] Training (Overparametrized) Neural Networks in Near-Linear Time. Manuscript. <a href="https://arxiv.org/pdf/2006.11648.pdf">[arXiv]</a> Jan van den Brand, Binghui Peng, Zhao Song and Omri Weinstein </li>
      <li> [JLSW20] An Improved Cutting Plane Method for Convex Optimization, Convex-Concave Games and its Applications. STOC 2020. <a href="https://arxiv.org/pdf/2004.04250.pdf"> [arXiv] </a> <a href="https://www.youtube.com/watch?v=ZX2BCc-LpNM">[Zhao's talk]</a> <a href="https://www.youtube.com/watch?v=3hdL8LEEez8">[Haotian's talk]</a> Haotian Jiang, Yin Tat Lee, Zhao Song and Sam Chiu-wai Wong 
      <li> [JSWZ20] Faster Dynamic Matrix Inverse for Faster LPs. Manuscript. <a href="https://arixv.org/pdf/2004.07470.pdf">[arXiv]</a> Shunhua Jiang, Zhao Song, Omri Weinstein and Hengjie Zhang</li>
      <li> [ACSS20] Algorithms and Hardness for Linear Algebra on Geometric Graphs. FOCS 2020 [arXiv] <a href="https://www.youtube.com/watch?v=0A96KUDq1pU">[Aaron's talk]</a> <a href="https://www.youtube.com/watch?v=TFspM_iKPQY">[Zhao's talk]</a> Josh Alman, Timothy Chu, Aaron Schild and Zhao Song </li>
    </ol>
    Security
    <ol>
      <li> [HSLA20] InstaHide: Instace-hiding Schemes for Private Distributed Learning. ICML 2020. [arXiv] <a href="https://proceedings.icml.cc/static/paper_files/icml/2020/2735-Paper.pdf">[conf]</a>. Yangsibo Huang, Zhao Song, Kai Li and Sanjeev Arora </li>
      <li> [HSR+20] Privacy-preserving Learning via Deep Net Pruning. Manuscript. <a href="https://arxiv.org/pdf/2003.01876.pdf">[arXiv]</a>. Yangsibo Huang, Yushan Su, Sachin Ravi, Zhao Song, Sanjeev Arora and Kai Li 
    </ol>
    Prune net
    <ol>
      <li> [HPTD15] Learning both weights and connections for efficient neural network. NIPS 2015. <a href="https://arxiv.org/pdf/1506.02626.pdf">[arXiv]</a> Song Han, Jeff Pool, John Tran and William Dally </li>
      <li> [HMD16] Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. ICLR 2016. <a href="https://arxiv.org/pdf/1510.00149.pdf">[arXiv]</a> Song Han, Huizi Mao and William J Dally </li>
    </ol>
    Adversarial
    <ol>
        <li> [KBD+17] Reluplex: An efficient SMT solver for verifying deep neural networks. CAV 2017. <a href="https://arxiv.org/pdf/1702.01135.pdf">[arXiv]</a> Guy Katz, Clark Barrett, David L Dill, Kyle Julian and Mykel J Kochenderfer</li>
        <li> [WK18] Provable defenses against adversarial examples via the convex outer adversarial polytope. ICML 2018. <a href="https://arxiv.org/pdf/1711.00851.pdf">[arXiv]</a> Eric Wong and Zico Kolter </li>
        <li> [WZC+18] Towards Fast Computation of Certified Robustness for ReLU Networks. ICML 2018. <a href="https://arxiv.org/pdf/1804.09699.pdf">[arXiv]</a> Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Duane Boning, Inderjit S Dhillon and Luca Daniel</li>
    </ol>

    Compressive sensing
    <ol>
      <li> [NS19] Stronger L2/L2 Compressed Sensing; Without Iterating. STOC 2019. <a href="https://arxiv.org/pdf/1903.02742.pdf">[arXiv]</a> Vasileios Nakos and Zhao Song </li>
      <li> [NSW19] (Nearly) Sample-Optimal Sparse Fourier Transform in Any Dimension; RIPless and Filterless. FOCS 2019. <a href="https://arxiv.org/pdf/1909.11123.pdf">[arXiv]</a> Vasileios Nakos, Zhao Song and Zhengyu Wang </li>
      <li> [JLS20] A robust multi-dimensional sparse Fourier transform in the continuous setting. Manuscript. <a href="https://arxiv.org/pdf/2005.06156.pdf">[arXiv]</a> Yaonan Jin, Daogao Liu and Zhao Song </li>
      <li> [LN20] Sublinear-Time Algorithms for Compressive Phase Retrieval. ITIT 2020. <a href ="https://arxiv.org/pdf/1709.02917.pdf">[arXiv]</a> Yi Li and Vasileios Nakos</li>
    </ol>
  </td>
</tr>

<tr valign="top">
    <td><b>
Students with <br>
Disabilites:
    </b></td>  
    <td>Any student with a documented disability (physical or
    cognitive) who requires academic accommodations should contact the
    Services for Students with Disabilities area of the Office of the
    Dean of Students at 471-6259 (voice) or 471-4641 (TTY for users
    who are deaf or hard of hearing) as soon as possible to request an
    official letter outlining authorized accommodations.
</td></tr>




</tbody></table>

<hr>
</body></html>